{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309336e7-7cf3-4e07-b7dd-d83f24c16df8",
   "metadata": {},
   "source": [
    "# Code Splitter (using ANTLR)\n",
    "\n",
    "## Prerequirements\n",
    "Install Java Runtime:\n",
    "```\n",
    "apt update\n",
    "apt install openjdk-11-jre\n",
    "apt install openjdk-11-jdk\n",
    "```\n",
    "\n",
    "## Useful Links\n",
    "- ANTLR Grammars: https://github.com/antlr/grammars-v4 (start rule can be found in `pom.xml` -> `entryPoint`)\n",
    "- ANTLR with Python runtime: https://github.com/antlr/antlr4/blob/master/doc/python-target.md\n",
    "- More examples for using ANTLR with Python: https://github.com/jszheng/py3antlr4book\n",
    "\n",
    "## Related\n",
    "- Code Splitter in LangChain [Docs](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter), [Code](https://github.com/langchain-ai/langchain/blob/b01a443ee525e274335f475a849a1681240ff249/libs/langchain/langchain/text_splitter.py#L816)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d44b1-d131-4f0a-8ce8-64ba9d1bfb04",
   "metadata": {},
   "source": [
    "Download ANTLR tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f2621cd-2a10-4da8-827d-990ff945daec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-20 15:06:48--  https://www.antlr.org/download/antlr-4.13.0-complete.jar\n",
      "Resolving www.antlr.org (www.antlr.org)... 2606:50c0:8001::153, 2606:50c0:8003::153, 2606:50c0:8002::153, ...\n",
      "Connecting to www.antlr.org (www.antlr.org)|2606:50c0:8001::153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2148972 (2.0M) [application/java-archive]\n",
      "Saving to: ‘antlr-4.13.0-complete.jar.1’\n",
      "\n",
      "antlr-4.13.0-comple 100%[===================>]   2.05M  --.-KB/s    in 0.008s  \n",
      "\n",
      "2023-10-20 15:06:49 (247 MB/s) - ‘antlr-4.13.0-complete.jar.1’ saved [2148972/2148972]\n",
      "\n",
      "--2023-10-20 15:06:49--  https://raw.githubusercontent.com/antlr/grammars-v4/master/c/C.g4\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8000::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17617 (17K) [text/plain]\n",
      "Saving to: ‘C.g4.1’\n",
      "\n",
      "C.g4.1              100%[===================>]  17.20K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-10-20 15:06:49 (93.9 MB/s) - ‘C.g4.1’ saved [17617/17617]\n",
      "\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.13.0 in ./venv/lib/python3.10/site-packages (4.13.0)\n",
      "context [/report CANNOT_WRITE_FILE] 1:19 attribute arg isn't defined\n",
      "error(1):  cannot write file : \n",
      "error(1):  cannot write file CParser.py: \n",
      "error(1):  cannot write file C.tokens: \n"
     ]
    }
   ],
   "source": [
    "# Install ANTLR Python runtime\n",
    "!pip install antlr4-python3-runtime==4.13.0\n",
    "# Download ANTLR tool\n",
    "!wget https://www.antlr.org/download/antlr-4.13.0-complete.jar\n",
    "# Download ANTLR grammar for C language\n",
    "!wget https://raw.githubusercontent.com/antlr/grammars-v4/master/c/C.g4\n",
    "# Generate C language parser in Python (we don't need .jar file after it)\n",
    "!java -jar ./antlr-4.13.0-complete.jar -Dlanguage=Python3 C.g4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "905f673a-dc43-435f-8e8c-bb4c374729f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-25 15:57:57--  https://raw.githubusercontent.com/postgres/postgres/master/src/backend/storage/large_object/inv_api.c\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 25210 (25K) [text/plain]\n",
      "Saving to: ‘inv_api.c’\n",
      "\n",
      "inv_api.c           100%[===================>]  24.62K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-10-25 15:57:57 (162 MB/s) - ‘inv_api.c’ saved [25210/25210]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test installation\n",
    "!wget https://raw.githubusercontent.com/postgres/postgres/master/src/backend/storage/large_object/inv_api.c\n",
    "!mv -f inv_api.c input.c\n",
    "#!pygrun C compilationUnit --tokens input.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da689c58-5eaf-418c-8639-7468c6b4f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import antlr4\n",
    "from antlr4 import *\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca6a95b0-64c5-475c-ad5a-1289b76327e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLexer import CLexer\n",
    "from CParser import CParser\n",
    "from CListener import CListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b153d180-6b72-4dff-83a1-93d494cb4cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "line 469:58 no viable alternative at input 'ereport(ERROR,\\n\\t\\t\\t\\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\\n\\t\\t\\t\\t errmsg_internal(\"invalid large object seek target: \" INT64_FORMAT'\n",
      "line 469:58 no viable alternative at input '(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\\n\\t\\t\\t\\t errmsg_internal(\"invalid large object seek target: \" INT64_FORMAT'\n",
      "line 469:58 no viable alternative at input 'errmsg_internal(\"invalid large object seek target: \" INT64_FORMAT'\n",
      "line 469:58 missing ';' at 'INT64_FORMAT'\n",
      "line 470:18 mismatched input ')' expecting ';'\n",
      "line 819:64 no viable alternative at input 'ereport(ERROR,\\n\\t\\t\\t\\t(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\\n\\t\\t\\t\\t errmsg_internal(\"invalid large object truncation target: \" INT64_FORMAT'\n",
      "line 819:64 no viable alternative at input '(errcode(ERRCODE_INVALID_PARAMETER_VALUE),\\n\\t\\t\\t\\t errmsg_internal(\"invalid large object truncation target: \" INT64_FORMAT'\n",
      "line 819:64 no viable alternative at input 'errmsg_internal(\"invalid large object truncation target: \" INT64_FORMAT'\n",
      "line 819:64 missing ';' at 'INT64_FORMAT'\n",
      "line 820:12 mismatched input ')' expecting ';'\n"
     ]
    }
   ],
   "source": [
    "input_stream = FileStream('/home/ubuntu/postgres-bot/input.c')\n",
    "lexer = CLexer(input_stream)\n",
    "stream = CommonTokenStream(lexer)\n",
    "parser = CParser(stream)\n",
    "tree = parser.compilationUnit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b05952b-2920-487f-9683-6bd27a8c9da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['compilationUnit',\n",
       " '  translationUnit',\n",
       " '    externalDeclaration',\n",
       " '      declaration',\n",
       " '        declarationSpecifiers',\n",
       " '          declarationSpecifier',\n",
       " '            typeSpecifier',\n",
       " '              typedefName',\n",
       " '                bool',\n",
       " '          declarationSpecifier',\n",
       " '            typeSpecifier',\n",
       " '              typedefName',\n",
       " '                lo_compat_privileges',\n",
       " '        ;',\n",
       " '    externalDeclaration',\n",
       " '      declaration',\n",
       " '        declarationSpecifiers',\n",
       " '          declarationSpecifier',\n",
       " '            storageClassSpecifier',\n",
       " '              static']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Tree Plot\n",
    "from antlr4.tree.Tree import TerminalNode\n",
    "\n",
    "class TreePrinter():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tree_strs = []\n",
    "\n",
    "    def print(self, tree, rule_names, indent=0):\n",
    "        if tree is None:\n",
    "            return\n",
    "    \n",
    "        if isinstance(tree, TerminalNode):\n",
    "            self.tree_strs.append(f\"{' ' * indent}{tree.getText()}\")\n",
    "            return\n",
    "    \n",
    "        rule_name = rule_names[tree.getRuleIndex()] if tree.getRuleIndex() >= 0 else \"Unknown\"\n",
    "    \n",
    "        self.tree_strs.append(f\"{' ' * indent}{rule_name}\")\n",
    "\n",
    "        if tree.children is None:\n",
    "            return\n",
    "\n",
    "        for child in tree.children:\n",
    "            self.print(child, rule_names, indent + 2)\n",
    "\n",
    "tree_printer = TreePrinter()\n",
    "tree_printer.print(tree, parser.ruleNames)\n",
    "tree_printer.tree_strs[0:min(20, len(tree_printer.tree_strs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32edaba-a881-4966-8ca2-b850edcdcf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['startRule', 'primaryExpression', 'genericSelection', 'genericAssocList', 'genericAssociation', 'postfixExpression', 'argumentExpressionList', 'unaryExpression', 'unaryOperator', 'castExpression', 'multiplicativeExpression', 'additiveExpression', 'shiftExpression', 'relationalExpression', 'equalityExpression', 'andExpression', 'exclusiveOrExpression', 'inclusiveOrExpression', 'logicalAndExpression', 'logicalOrExpression', 'conditionalExpression', 'assignmentExpression', 'assignmentOperator', 'expression', 'constantExpression', 'declaration', 'declarationSpecifiers', 'declarationSpecifiers2', 'declarationSpecifier', 'initDeclaratorList', 'initDeclarator', 'storageClassSpecifier', 'typeSpecifier', 'structOrUnionSpecifier', 'structOrUnion', 'structDeclarationList', 'structDeclaration', 'specifierQualifierList', 'structDeclaratorList', 'structDeclarator', 'enumSpecifier', 'enumeratorList', 'enumerator', 'enumerationConstant', 'atomicTypeSpecifier', 'typeQualifier', 'functionSpecifier', 'alignmentSpecifier', 'declarator', 'directDeclarator', 'vcSpecificModifer', 'gccDeclaratorExtension', 'gccAttributeSpecifier', 'gccAttributeList', 'gccAttribute', 'nestedParenthesesBlock', 'pointer', 'typeQualifierList', 'parameterTypeList', 'parameterList', 'parameterDeclaration', 'identifierList', 'typeName', 'abstractDeclarator', 'directAbstractDeclarator', 'typedefName', 'initializer', 'initializerList', 'designation', 'designatorList', 'designator', 'staticAssertDeclaration', 'statement', 'labeledStatement', 'compoundStatement', 'blockItemList', 'blockItem', 'expressionStatement', 'selectionStatement', 'iterationStatement', 'forCondition', 'forDeclaration', 'forExpression', 'jumpStatement', 'compilationUnit', 'translationUnit', 'externalDeclaration', 'functionDefinition', 'declarationList']\n"
     ]
    }
   ],
   "source": [
    "# Parser: AST node types\n",
    "print(parser.ruleNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08063ebe-ae66-4eda-8b4a-5d3f7e9733d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EMPTY', 'EOF', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', 'accept', 'addChild', 'addErrorNode', 'addTokenNode', 'children', 'copyFrom', 'depth', 'enterRule', 'exception', 'exitRule', 'getAltNumber', 'getChild', 'getChildCount', 'getChildren', 'getPayload', 'getRuleContext', 'getRuleIndex', 'getSourceInterval', 'getText', 'getToken', 'getTokens', 'getTypedRuleContext', 'getTypedRuleContexts', 'invokingState', 'isEmpty', 'parentCtx', 'parser', 'removeLastChild', 'setAltNumber', 'start', 'stop', 'toString', 'toStringTree', 'translationUnit']\n"
     ]
    }
   ],
   "source": [
    "# AST node fields\n",
    "print(dir(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93650db1-dd28-4331-8089-b1aba468148f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[@0,0:1371='/*-------------------------------------------------------------------------\\n *\\n * inv_api.c\\n *\\t  routines for manipulating inversion fs large objects. This file\\n *\\t  contains the user-level large object application interface routines.\\n *\\n *\\n * Note: we access pg_largeobject.data using its C struct declaration.\\n * This is safe because it immediately follows pageno which is an int4 field,\\n * and therefore the data field will always be 4-byte aligned, even if it\\n * is in the short 1-byte-header format.  We have to detoast it since it's\\n * quite likely to be in compressed or short format.  We also need to check\\n * for NULLs, since initdb will mark loid and pageno but not data as NOT NULL.\\n *\\n * Note: many of these routines leak memory in CurrentMemoryContext, as indeed\\n * does most of the backend code.  We expect that CurrentMemoryContext will\\n * be a short-lived context.  Data that must persist across function calls\\n * is kept either in CacheMemoryContext (the Relation structs) or in the\\n * memory context given to inv_open (for LargeObjectDesc structs).\\n *\\n *\\n * Portions Copyright (c) 1996-2023, PostgreSQL Global Development Group\\n * Portions Copyright (c) 1994, Regents of the University of California\\n *\\n *\\n * IDENTIFICATION\\n *\\t  src/backend/storage/large_object/inv_api.c\\n *\\n *-------------------------------------------------------------------------\\n */',<119>,channel=1,1:0]\n",
      "[@1,1372:1372='\\n',<118>,channel=1,30:3]\n",
      "[@2,1373:1393='#include \"postgres.h\"',<115>,channel=1,31:0]\n",
      "[@3,1394:1394='\\n',<118>,channel=1,31:21]\n",
      "[@4,1395:1395='\\n',<118>,channel=1,32:0]\n",
      "[@5,1396:1414='#include <limits.h>',<115>,channel=1,33:0]\n",
      "[@6,1415:1415='\\n',<118>,channel=1,33:19]\n",
      "[@7,1416:1416='\\n',<118>,channel=1,34:0]\n",
      "[@8,1417:1443='#include \"access/detoast.h\"',<115>,channel=1,35:0]\n",
      "[@9,1444:1444='\\n',<118>,channel=1,35:27]\n",
      "[@10,1445:1469='#include \"access/genam.h\"',<115>,channel=1,36:0]\n",
      "[@11,1470:1470='\\n',<118>,channel=1,36:25]\n",
      "[@12,1471:1502='#include \"access/htup_details.h\"',<115>,channel=1,37:0]\n",
      "[@13,1503:1503='\\n',<118>,channel=1,37:32]\n",
      "[@14,1504:1530='#include \"access/sysattr.h\"',<115>,channel=1,38:0]\n",
      "[@15,1531:1531='\\n',<118>,channel=1,38:27]\n",
      "[@16,1532:1556='#include \"access/table.h\"',<115>,channel=1,39:0]\n",
      "[@17,1557:1557='\\n',<118>,channel=1,39:25]\n",
      "[@18,1558:1581='#include \"access/xact.h\"',<115>,channel=1,40:0]\n",
      "[@19,1582:1582='\\n',<118>,channel=1,40:24]\n"
     ]
    }
   ],
   "source": [
    "input_stream = FileStream('/home/ubuntu/postgres-bot/input.c')\n",
    "lexer = CLexer(input_stream)\n",
    "token_stream = CommonTokenStream(lexer)\n",
    "token_stream.fill()\n",
    "\n",
    "for tok in token_stream.tokens[0:min(20, len(token_stream.tokens))]:\n",
    "    print(tok)\n",
    "\n",
    "# Note: 119 - BlockComment, 120 - LineComment token types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcabe38d-fbec-454f-ba9f-2198ac26efe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token text: #include \"access/genam.h\"\n",
      "Token line: 36\n",
      "Token index: 10\n",
      "Token type: 115\n",
      "Token source: <CLexer.CLexer object at 0x7fe410d3ea40>\n"
     ]
    }
   ],
   "source": [
    "token = token_stream.tokens[10]\n",
    "print('Token text:', token.text)\n",
    "print('Token line:', token.line)\n",
    "print('Token index:', token.tokenIndex)\n",
    "print('Token type:', token.type)\n",
    "print('Token source:', token.getTokenSource())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dcbe2a6-a4c1-4ad3-bb34-de7d8a298fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEFAULT_CHANNEL',\n",
       " 'EMPTY_SOURCE',\n",
       " 'EOF',\n",
       " 'EPSILON',\n",
       " 'HIDDEN_CHANNEL',\n",
       " 'INVALID_TYPE',\n",
       " 'MIN_USER_TOKEN_TYPE',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_text',\n",
       " 'channel',\n",
       " 'clone',\n",
       " 'column',\n",
       " 'getInputStream',\n",
       " 'getTokenSource',\n",
       " 'line',\n",
       " 'source',\n",
       " 'start',\n",
       " 'stop',\n",
       " 'text',\n",
       " 'tokenIndex',\n",
       " 'type']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token_stream.tokens[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eaaef920-c8ac-4327-aba7-eec86bf30e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuncLineCollector(ParseTreeListener):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.func_lines = []\n",
    "        self.func_start = -1\n",
    "        self.depth = 0\n",
    "\n",
    "    def enterFunctionDefinition(self, ctx):\n",
    "        self.func_start = ctx.start.line\n",
    "        self.depth = 0\n",
    "\n",
    "    def enterCompoundStatement(self, ctx):\n",
    "        self.depth += 1\n",
    "\n",
    "    def exitCompoundStatement(self, ctx):\n",
    "        self.depth -= 1\n",
    "        if self.depth == 0:\n",
    "            func_end = ctx.stop.line\n",
    "            self.func_lines.append((self.func_start, func_end))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "595e325b-e486-4792-b0db-24da5ec58a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 functions:\n",
      "Function on lines 74 to 93\n",
      "Function on lines 98 to 124\n",
      "Function on lines 131 to 161\n",
      "Function on lines 169 to 196\n",
      "Function on lines 211 to 242\n",
      "Function on lines 254 to 332\n",
      "Function on lines 338 to 343\n",
      "Function on lines 350 to 371\n",
      "Function on lines 379 to 425\n",
      "Function on lines 427 to 474\n",
      "Function on lines 476 to 487\n",
      "Function on lines 489 to 580\n",
      "Function on lines 582 to 777\n",
      "Function on lines 779 to 955\n"
     ]
    }
   ],
   "source": [
    "listener = FuncLineCollector()\n",
    "ParseTreeWalker().walk(listener, tree)\n",
    "\n",
    "print(f\"Found {len(listener.func_lines)} functions:\")\n",
    "for start, end in listener.func_lines:\n",
    "    print(f\"Function on lines {start} to {end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56104ecd-d7ce-4497-a55f-f5e8ce8aedf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3132"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('\\n'.join(input_stream.getText(0,10000).split('\\n')[0:93]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78c4dc6d-2d72-4ae1-89f7-557de3d008f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines_in_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return sum(1 for line in file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71a5ecff-6f3a-4dc6-a32e-d9acb2125aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "numLines = count_lines_in_file('/home/ubuntu/postgres-bot/input.c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e44ad385-3407-4027-a53f-22384f2664dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment on lines: 1 30 \n",
      " /*-------------------------------------------------------------------------\n",
      " *\n",
      " * inv_api.c\n",
      " *\t  routines for manipulating inversion fs large objects. This file\n",
      " *\t  contains the user-level large object application interface routines.\n",
      " *\n",
      " *\n",
      " * Note: we access pg_largeobject.data using its C struct declaration.\n",
      " * This is safe because it immediately follows pageno which is an int4 field,\n",
      " * and therefore the data field will always be 4-byte aligned, even if it\n",
      " * is in the short 1-byte-header format.  We have to detoast it since it's\n",
      " * quite likely to be in compressed or short format.  We also need to check\n",
      " * for NULLs, since initdb will mark loid and pageno but not data as NOT NULL.\n",
      " *\n",
      " * Note: many of these routines leak memory in CurrentMemoryContext, as indeed\n",
      " * does most of the backend code.  We expect that CurrentMemoryContext will\n",
      " * be a short-lived context.  Data that must persist across function calls\n",
      " * is kept either in CacheMemoryContext (the Relation structs) or in the\n",
      " * memory context given to inv_open (for LargeObjectDesc structs).\n",
      " *\n",
      " *\n",
      " * Portions Copyright (c) 1996-2023, PostgreSQL Global Development Group\n",
      " * Portions Copyright (c) 1994, Regents of the University of California\n",
      " *\n",
      " *\n",
      " * IDENTIFICATION\n",
      " *\t  src/backend/storage/large_object/inv_api.c\n",
      " *\n",
      " *-------------------------------------------------------------------------\n",
      " */ \n",
      "\n",
      "Comment on lines: 55 57 \n",
      " /*\n",
      " * GUC: backwards-compatibility flag to suppress LO permission checks\n",
      " */ \n",
      "\n",
      "Comment on lines: 60 66 \n",
      " /*\n",
      " * All accesses to pg_largeobject and its index make use of a single\n",
      " * Relation reference.  To guarantee that the relcache entry remains\n",
      " * in the cache, on the first reference inside a subtransaction, we\n",
      " * execute a slightly klugy maneuver to assign ownership of the\n",
      " * Relation reference to TopTransactionResourceOwner.\n",
      " */ \n",
      "\n",
      "Comment on lines: 71 73 \n",
      " /*\n",
      " * Open pg_largeobject and its index, if not already done in current xact\n",
      " */ \n",
      "\n",
      "Comment on lines: 80 81 \n",
      " /* already open in current xact */ \n",
      "\n",
      "Comment on lines: 82 83 \n",
      " /* Arrange for the top xact to own these relation references */ \n",
      "\n",
      "Comment on lines: 86 87 \n",
      " /* Use RowExclusiveLock since we might either read or write */ \n",
      "\n",
      "Comment on lines: 95 97 \n",
      " /*\n",
      " * Clean up at main transaction end\n",
      " */ \n",
      "\n",
      "Comment on lines: 103 106 \n",
      " /*\n",
      "\t\t * Only bother to close if committing; else abort cleanup will handle\n",
      "\t\t * it\n",
      "\t\t */ \n",
      "\n",
      "Comment on lines: 127 130 \n",
      " /*\n",
      " * Same as pg_largeobject.c's LargeObjectExists(), except snapshot to\n",
      " * read with can be specified.\n",
      " */ \n",
      "\n",
      "Comment on lines: 164 168 \n",
      " /*\n",
      " * Extract data field from a pg_largeobject tuple, detoasting if needed\n",
      " * and verifying that the length is sane.  Returns data pointer (a bytea *),\n",
      " * data length, and an indication of whether to pfree the data pointer.\n",
      " */ \n",
      "\n",
      "Comment on lines: 179 180 \n",
      " /* see note at top of file */ \n",
      "\n",
      "Comment on lines: 199 210 \n",
      " /*\n",
      " *\tinv_create -- create a new large object\n",
      " *\n",
      " *\tArguments:\n",
      " *\t  lobjId - OID to use for new large object, or InvalidOid to pick one\n",
      " *\n",
      " *\tReturns:\n",
      " *\t  OID of new object\n",
      " *\n",
      " * If lobjId is not InvalidOid, then an error occurs if the OID is already\n",
      " * in use.\n",
      " */ \n",
      "\n",
      "Comment on lines: 216 218 \n",
      " /*\n",
      "\t * Create a new largeobject with empty data pages\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 221 229 \n",
      " /*\n",
      "\t * dependency on the owner of largeobject\n",
      "\t *\n",
      "\t * The reason why we use LargeObjectRelationId instead of\n",
      "\t * LargeObjectMetadataRelationId here is to provide backward compatibility\n",
      "\t * to the applications which utilize a knowledge about internal layout of\n",
      "\t * system catalogs. OID of pg_largeobject_metadata and loid of\n",
      "\t * pg_largeobject are same value, so there are no actual differences here.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 233 234 \n",
      " /* Post creation hook for new large object */ \n",
      "\n",
      "Comment on lines: 236 238 \n",
      " /*\n",
      "\t * Advance command counter to make new tuple visible to later operations.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 244 253 \n",
      " /*\n",
      " *\tinv_open -- access an existing large object.\n",
      " *\n",
      " * Returns a large object descriptor, appropriately filled in.\n",
      " * The descriptor and subsidiary data are allocated in the specified\n",
      " * memory context, which must be suitably long-lived for the caller's\n",
      " * purposes.  If the returned descriptor has a snapshot associated\n",
      " * with it, the caller must ensure that it also lives long enough,\n",
      " * e.g. by calling RegisterSnapshotOnOwner\n",
      " */ \n",
      "\n",
      "Comment on lines: 261 265 \n",
      " /*\n",
      "\t * Historically, no difference is made between (INV_WRITE) and (INV_WRITE\n",
      "\t * | INV_READ), the caller being allowed to read the large object\n",
      "\t * descriptor in either case.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 277 278 \n",
      " /* Get snapshot.  If write is requested, use an instantaneous snapshot. */ \n",
      "\n",
      "Comment on lines: 283 284 \n",
      " /* Can't use LargeObjectExists here because we need to specify snapshot */ \n",
      "\n",
      "Comment on lines: 289 290 \n",
      " /* Apply permission checks, again specifying snapshot */ \n",
      "\n",
      "Comment on lines: 315 316 \n",
      " /* OK to create a descriptor */ \n",
      "\n",
      "Comment on lines: 322 323 \n",
      " /* caller sets if needed, not used by the functions in this file */ \n",
      "\n",
      "Comment on lines: 325 328 \n",
      " /*\n",
      "\t * The snapshot (if any) is just the currently active snapshot.  The\n",
      "\t * caller will replace it with a longer-lived copy if needed.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 334 337 \n",
      " /*\n",
      " * Closes a large object descriptor previously made by inv_open(), and\n",
      " * releases the long-term memory used by it.\n",
      " */ \n",
      "\n",
      "Comment on lines: 345 349 \n",
      " /*\n",
      " * Destroys an existing large object (not to be confused with a descriptor!)\n",
      " *\n",
      " * Note we expect caller to have done any required permissions check.\n",
      " */ \n",
      "\n",
      "Comment on lines: 355 357 \n",
      " /*\n",
      "\t * Delete any comments and dependencies on the large object\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 363 366 \n",
      " /*\n",
      "\t * Advance command counter so that tuple removal will be seen by later\n",
      "\t * large-object operations in this transaction.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 369 370 \n",
      " /* For historical reasons, we always return 1 on success. */ \n",
      "\n",
      "Comment on lines: 373 378 \n",
      " /*\n",
      " * Determine size of a large object\n",
      " *\n",
      " * NOTE: LOs can contain gaps, just like Unix files.  We actually return\n",
      " * the offset of the last byte + 1.\n",
      " */ \n",
      "\n",
      "Comment on lines: 399 404 \n",
      " /*\n",
      "\t * Because the pg_largeobject index is on both loid and pageno, but we\n",
      "\t * constrain only loid, a backwards scan should visit all pages of the\n",
      "\t * large object in reverse pageno order.  So, it's sufficient to examine\n",
      "\t * the first valid tuple (== last valid page).\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 413 414 \n",
      " /* paranoia */ \n",
      "\n",
      "Comment on lines: 434 437 \n",
      " /*\n",
      "\t * We allow seek/tell if you have either read or write permission, so no\n",
      "\t * need for a permission check here.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 439 442 \n",
      " /*\n",
      "\t * Note: overflow in the additions is possible, but since we will reject\n",
      "\t * negative results, we don't need any extra test for that.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 458 459 \n",
      " /* keep compiler quiet */ \n",
      "\n",
      "Comment on lines: 462 465 \n",
      " /*\n",
      "\t * use errmsg_internal here because we don't want to expose INT64_FORMAT\n",
      "\t * in translatable strings; doing better is not worth the trouble\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 481 484 \n",
      " /*\n",
      "\t * We allow seek/tell if you have either read or write permission, so no\n",
      "\t * need for a permission check here.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 535 536 \n",
      " /* paranoia */ \n",
      "\n",
      "Comment on lines: 539 543 \n",
      " /*\n",
      "\t\t * We expect the indexscan will deliver pages in order.  However,\n",
      "\t\t * there may be missing pages if the LO contains unwritten \"holes\". We\n",
      "\t\t * want missing sections to read out as zeroes.\n",
      "\t\t */ \n",
      "\n",
      "Comment on lines: 600 601 \n",
      " /* this is to make the union big enough for a LO data chunk: */ \n",
      "\n",
      "Comment on lines: 602 603 \n",
      " /* ensure union is aligned well enough: */ \n",
      "\n",
      "Comment on lines: 615 616 \n",
      " /* enforce writability because snapshot is probably wrong otherwise */ \n",
      "\n",
      "Comment on lines: 625 626 \n",
      " /* this addition can't overflow because nbytes is only int32 */ \n",
      "\n",
      "Comment on lines: 655 658 \n",
      " /*\n",
      "\t\t * If possible, get next pre-existing page of the LO.  We expect the\n",
      "\t\t * indexscan will deliver these in order --- but there may be holes.\n",
      "\t\t */ \n",
      "\n",
      "Comment on lines: 663 664 \n",
      " /* paranoia */ \n",
      "\n",
      "Comment on lines: 671 674 \n",
      " /*\n",
      "\t\t * If we have a pre-existing page, see if it is the page we want to\n",
      "\t\t * write, or a later one.\n",
      "\t\t */ \n",
      "\n",
      "Comment on lines: 677 681 \n",
      " /*\n",
      "\t\t\t * Update an existing page with fresh data.\n",
      "\t\t\t *\n",
      "\t\t\t * First, load old data into workbuf\n",
      "\t\t\t */ \n",
      "\n",
      "Comment on lines: 687 689 \n",
      " /*\n",
      "\t\t\t * Fill any hole\n",
      "\t\t\t */ \n",
      "\n",
      "Comment on lines: 694 696 \n",
      " /*\n",
      "\t\t\t * Insert appropriate portion of new data\n",
      "\t\t\t */ \n",
      "\n",
      "Comment on lines: 703 704 \n",
      " /* compute valid length of new page */ \n",
      "\n",
      "Comment on lines: 707 709 \n",
      " /*\n",
      "\t\t\t * Form and insert updated tuple\n",
      "\t\t\t */ \n",
      "\n",
      "Comment on lines: 721 723 \n",
      " /*\n",
      "\t\t\t * We're done with this old page.\n",
      "\t\t\t */ \n",
      "\n",
      "Comment on lines: 730 734 \n",
      " /*\n",
      "\t\t\t * Write a brand new page.\n",
      "\t\t\t *\n",
      "\t\t\t * First, fill any hole\n",
      "\t\t\t */ \n",
      "\n",
      "Comment on lines: 739 741 \n",
      " /*\n",
      "\t\t\t * Insert appropriate portion of new data\n",
      "\t\t\t */ \n",
      "\n",
      "Comment on lines: 747 748 \n",
      " /* compute valid length of new page */ \n",
      "\n",
      "Comment on lines: 751 753 \n",
      " /*\n",
      "\t\t\t * Form and insert updated tuple\n",
      "\t\t\t */ \n",
      "\n",
      "Comment on lines: 770 773 \n",
      " /*\n",
      "\t * Advance command counter so that my tuple updates will be seen by later\n",
      "\t * large-object operations in this transaction.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 791 792 \n",
      " /* this is to make the union big enough for a LO data chunk: */ \n",
      "\n",
      "Comment on lines: 793 794 \n",
      " /* ensure union is aligned well enough: */ \n",
      "\n",
      "Comment on lines: 805 806 \n",
      " /* enforce writability because snapshot is probably wrong otherwise */ \n",
      "\n",
      "Comment on lines: 812 815 \n",
      " /*\n",
      "\t * use errmsg_internal here because we don't want to expose INT64_FORMAT\n",
      "\t * in translatable strings; doing better is not worth the trouble\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 826 828 \n",
      " /*\n",
      "\t * Set up to find all pages with desired loid and pageno >= target\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 842 845 \n",
      " /*\n",
      "\t * If possible, get the page the truncation point is in. The truncation\n",
      "\t * point may be beyond the end of the LO or in a hole.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 849 850 \n",
      " /* paranoia */ \n",
      "\n",
      "Comment on lines: 855 859 \n",
      " /*\n",
      "\t * If we found the page of the truncation point we need to truncate the\n",
      "\t * data in it.  Otherwise if we're in a hole, we need to create a page to\n",
      "\t * mark the end of data.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 862 863 \n",
      " /* First, load old data into workbuf */ \n",
      "\n",
      "Comment on lines: 872 874 \n",
      " /*\n",
      "\t\t * Fill any hole\n",
      "\t\t */ \n",
      "\n",
      "Comment on lines: 879 880 \n",
      " /* compute length of new page */ \n",
      "\n",
      "Comment on lines: 882 884 \n",
      " /*\n",
      "\t\t * Form and insert updated tuple\n",
      "\t\t */ \n",
      "\n",
      "Comment on lines: 898 902 \n",
      " /*\n",
      "\t\t * If the first page we found was after the truncation point, we're in\n",
      "\t\t * a hole that we'll fill, but we need to delete the later page\n",
      "\t\t * because the loop below won't visit it again.\n",
      "\t\t */ \n",
      "\n",
      "Comment on lines: 909 913 \n",
      " /*\n",
      "\t\t * Write a brand new page.\n",
      "\t\t *\n",
      "\t\t * Fill the hole up to the truncation point\n",
      "\t\t */ \n",
      "\n",
      "Comment on lines: 918 919 \n",
      " /* compute length of new page */ \n",
      "\n",
      "Comment on lines: 921 923 \n",
      " /*\n",
      "\t\t * Form and insert new tuple\n",
      "\t\t */ \n",
      "\n",
      "Comment on lines: 934 937 \n",
      " /*\n",
      "\t * Delete any pages after the truncation point.  If the initial search\n",
      "\t * didn't find a page, then of course there's nothing more to do.\n",
      "\t */ \n",
      "\n",
      "Comment on lines: 950 953 \n",
      " /*\n",
      "\t * Advance command counter so that tuple updates will be seen by later\n",
      "\t * large-object operations in this transaction.\n",
      "\t */ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = token_stream.tokens\n",
    "comments = []\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    if token.type in [119, 120]:\n",
    "        token.lineStart = token.line\n",
    "        token.lineEnd = token.line  # Default value\n",
    "        \n",
    "        # Assume the comment extends to the end of the file if no subsequent token is found\n",
    "        token.lineEnd = numLines\n",
    "\n",
    "        # Find the next token that is on a different line\n",
    "        for next_token in tokens[i+1:]:\n",
    "            if next_token.line > token.line:\n",
    "                token.lineEnd = next_token.line\n",
    "                break\n",
    "\n",
    "        comments.append(token)\n",
    "\n",
    "for tok in comments:\n",
    "    print('Comment on lines:', tok.lineStart, tok.lineEnd, '\\n', tok.text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2063a8ab-6079-4853-a3e5-50090c149de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function on lines 71 to 93\n",
      "Function on lines 95 to 124\n",
      "Function on lines 127 to 161\n",
      "Function on lines 164 to 196\n",
      "Function on lines 199 to 242\n",
      "Function on lines 244 to 332\n",
      "Function on lines 334 to 343\n",
      "Function on lines 345 to 371\n",
      "Function on lines 373 to 425\n",
      "Function on lines 427 to 474\n",
      "Function on lines 476 to 487\n",
      "Function on lines 489 to 580\n",
      "Function on lines 582 to 777\n",
      "Function on lines 779 to 955\n"
     ]
    }
   ],
   "source": [
    "func_with_comments_lines = []\n",
    "\n",
    "for start, end in listener.func_lines:\n",
    "    # Find corresponding comment on previous line.\n",
    "    for comment in comments:\n",
    "        lineStart = comment.lineStart\n",
    "        lineEnd = comment.lineEnd\n",
    "\n",
    "        if start - 1 == lineEnd:\n",
    "            start = lineStart\n",
    "            break\n",
    "\n",
    "    print(f\"Function on lines {start} to {end}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
